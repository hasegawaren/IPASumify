from bs4 import BeautifulSoup
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List, Dict
import io
import re
import httpx
import os
import logging
import tiktoken
import pdfplumber
import fitz  
from PyPDF2 import PdfReader
from dotenv import load_dotenv
import uuid
import asyncio

# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

load_dotenv()

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    logging.error("‚ùå DeepSeek API Key is missing! Please set it in the .env file.")
    raise ValueError("DeepSeek API Key is missing! Please set it in the .env file.")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

sessions: Dict[str, Dict] = {}
MAX_TOKENS = 4000
MAX_PDF_SIZE_MB = 10

# ‚úÖ ‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢‡∏à‡∏≤‡∏Å PDF (Table of Contents)
def extract_pdf_toc(content: bytes) -> List[Dict[str, str]]:
    pdf_document = fitz.open(stream=content, filetype="pdf")
    toc = pdf_document.get_toc()
    pdf_document.close()
    
    return [{"title": entry[1], "page": entry[2]} for entry in toc] if toc else []

# ‚úÖ ‡πÉ‡∏ä‡πâ pdfplumber ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
async def extract_text_from_pdf(content: bytes) -> str:
    pdf_reader = PdfReader(io.BytesIO(content))
    extracted_texts = []

    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if not page_text:
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                try:
                    page_text = pdf.pages[page.page_number].extract_text()
                except IndexError:
                    page_text = ""
        extracted_texts.append(page_text or "")

    extracted_text = re.sub(r"\s+", " ", " ".join(extracted_texts).strip())
    return extracted_text

# ‚úÖ ‡πÉ‡∏ä‡πâ batch processing ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ API ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
async def get_deepseek_response_batch(chunks: List[str]) -> List[str]:
    tasks = [
        get_deepseek_response([{"role": "user", "content": f"‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ:\n\n{chunk}"}]) 
        for chunk in chunks
    ]
    return await asyncio.gather(*tasks)

async def get_deepseek_response(messages: List[Dict[str, str]]) -> str:
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏Ç‡∏≠‡∏á DeepSeek ‡πÅ‡∏ö‡∏ö async ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÅ‡∏•‡∏∞ retry"""
    deepseek_api_url = "https://api.deepseek.com/chat/completions"
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "deepseek-chat", "messages": messages}

    retries = 3
    for attempt in range(retries):
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Timeout ‡πÄ‡∏õ‡πá‡∏ô 60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                response = await client.post(deepseek_api_url, json=payload, headers=headers)
                response.raise_for_status()
                response_data = response.json()
                return response_data["choices"][0]["message"]["content"]
        except httpx.HTTPStatusError as e:
            if attempt < retries - 1:
                await asyncio.sleep(3)
            else:
                raise HTTPException(status_code=500, detail=f"DeepSeek API error: {e.response.text}")
async def fetch_wikipedia_content(wiki_url: str) -> Dict[str, str]:
    """ üîπ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Wikipedia ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ (TOC) """
    try:
        parsed_url = httpx.URL(wiki_url)
        domain_parts = parsed_url.host.split(".")
        if len(domain_parts) < 3 or domain_parts[1] != "wikipedia":
            raise ValueError("Invalid Wikipedia URL")

        lang_code = domain_parts[0] if domain_parts[0] != "www" else "en"
        page_title = wiki_url.split("/wiki/")[-1]

        wikipedia_api_url = f"https://{lang_code}.wikipedia.org/wiki/{page_title}"

        async with httpx.AsyncClient() as client:
            response = await client.get(wikipedia_api_url)
            response.raise_for_status()
            html_content = response.text

        # üîπ ‡πÉ‡∏ä‡πâ BeautifulSoup ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå HTML
        soup = BeautifulSoup(html_content, "html.parser")

        # ‚úÖ ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡πÜ
        paragraphs = [p.get_text().strip() for p in soup.select("div.mw-parser-output > p") if p.get_text().strip()]
        summary = " ".join(paragraphs[:3])  # ‡∏î‡∏∂‡∏á 3 ‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å

        # ‚úÖ ‡∏î‡∏∂‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ (TOC)
        toc_list = []
        exclude_list = ["‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç", "‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏", "‡∏î‡∏π‡πÄ‡∏û‡∏¥‡πà‡∏°", "‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á", "‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô"]
        for heading in soup.select("h2, h3"):
            heading_text = heading.get_text().strip().replace("[‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç]", "").replace("[edit]", "")
            if heading_text and heading_text not in exclude_list:
                toc_list.append(heading_text)
                
        return {
            "summary": summary if summary else "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Wikipedia ‡πÑ‡∏î‡πâ",
            "toc": toc_list,
            "html": html_content
        }

    except httpx.HTTPStatusError as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch Wikipedia content: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

    
@app.post("/api/summarize")
async def summarize(
    input_type: str = Form(...),
    user_text: str = Form(None),
    pdf_file: Optional[UploadFile] = File(None),
    wiki_url: Optional[str] = Form(None),
    session_id: str = Form(None)
):
    logging.info(f"üì© Received Request - input_type: {input_type}")

    if not session_id:
        session_id = str(uuid.uuid4())

    if session_id not in sessions:
        sessions[session_id] = {"context": None, "summary": None}

    article_text = ""
    toc = []  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢

    if input_type == "text":
        logging.info("üìù Processing Text Input")
        article_text = user_text or ""

    elif input_type == "pdf":
        logging.info("üìÑ Processing PDF File")
        if not pdf_file:
            raise HTTPException(status_code=400, detail="No PDF file uploaded.")
        
        content = await pdf_file.read()  # ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        pdf_toc = extract_pdf_toc(content)  # ‚úÖ ‡∏î‡∏∂‡∏á TOC ‡∏à‡∏≤‡∏Å PDF
        article_text = await extract_text_from_pdf(content)  # ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF

        # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ TOC ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô response
        if pdf_toc:
            toc = [entry["title"] for entry in pdf_toc]
            toc_text = "\n".join([f"{entry['title']} (page {entry['page']})" for entry in pdf_toc])
            article_text = f"üîπ **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢‡πÉ‡∏ô PDF:**\n{toc_text}\n\nüìÑ **‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å PDF:**\n{article_text[:1000]}..."  # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô

    elif input_type == "wiki":
        logging.info(f"üåç Fetching Wikipedia Summary from: {wiki_url}")
        if not wiki_url:
            raise HTTPException(status_code=400, detail="No wiki_url provided.")
        
        try:
            wiki_data = await fetch_wikipedia_content(wiki_url)  # ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Wikipedia
            article_text = wiki_data["summary"]
            toc = wiki_data["toc"]
            html_content = wiki_data["html"]
            sessions[session_id] = {
            "wiki_url": wiki_url,
            "html": html_content,
            "summary": article_text
        }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error fetching Wikipedia content: {str(e)}")

    else:
        raise HTTPException(status_code=400, detail="Invalid input_type. Use text/pdf/wiki")

    if not article_text.strip():
        raise HTTPException(status_code=400, detail="Extracted text is empty.")

    # ‚úÖ ‡∏™‡πà‡∏á summary + TOC ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
    return {
        "session_id": session_id,
        "summary": article_text,
        "toc": toc
    }
@app.post("/api/chat")
async def chat(
    session_id: str = Body(...),
    topic: Optional[str] = Body(None),
    question: Optional[str] = Body(None)
):
    session = sessions.get(session_id)
    if not session or "html" not in session:
        raise HTTPException(status_code=400, detail="Session or HTML content not found.")

    html_content = session["html"]
    soup = BeautifulSoup(html_content, "html.parser")
    content_div = soup.find("div", {"class": "mw-parser-output"})

    if topic:
        target_heading = None
        for heading in content_div.find_all(["h2", "h3"]):
            heading_text = heading.get_text().strip().replace("[‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç]", "").replace("[edit]", "")
            if heading_text == topic:
                target_heading = heading
                break
        else:
            raise HTTPException(status_code=400, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ Wikipedia")

        content = []
        for sibling in target_heading.find_next_siblings():
            if sibling.name in ["h2", "h3"]:
                break
            if sibling.name == "p":
                content.append(sibling.get_text().strip())

        topic_text = " ".join(content).strip()

        if not topic_text:
            topic_text = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ"

        answer = await get_deepseek_response([{
            "role": "user",
            "content": f"‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ '{topic}' ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{topic_text}"
        }])

        return {"answer": answer}
